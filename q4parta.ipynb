{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import linecache\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from decimal import Decimal, getcontext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parse the mtx data into dataX and dataY where dataX is the term and dataY is the type of article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(286774,)\n",
      "(286774,)\n",
      "[0 0 0 ... 4 4 4]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#for the emtpy matrix above, the rows represent the type of article and matrix[row][column] is the number of occurences of term #column in articles of type row\n",
    "# row 0 <=> business, row 1 <=> entertainment, row 2 <=> politics, row 3 <=> sport, row 4 <=> tech\n",
    "mtx = 'bbc.mtx'\n",
    "docs = 'bbc.docs'\n",
    "\n",
    "docs_file = open(docs,'r')\n",
    "docs_content = docs_file.read()\n",
    "docs_content = docs_content.split('\\n')\n",
    "dataX = []\n",
    "dataY = []\n",
    "with open(mtx,'r') as file:\n",
    "    #skip first two metadata lines\n",
    "    file.readline()\n",
    "    file.readline()\n",
    "\n",
    "    #now read rest of the file\n",
    " \n",
    "    for line in file:\n",
    "        '''Each row in bbc.mtx, except the first two, represents the frequency of a term in a given\n",
    "        article. For example, row 812 (“2 528\n",
    "        5.0”) indicates that term 2 (“sale”) occurs 5 times\n",
    "        in article 528 (entertainment.018).'''\n",
    "        words = line.split()\n",
    "        term = int(words[0])\n",
    "        article = int(words[1])\n",
    "\n",
    "        article_type = docs_content[article-1].split('.')[0]\n",
    "        article_type_lower = article_type.lower()  # Convert to lowercase for case-insensitivity\n",
    "\n",
    "        if article_type_lower == \"business\":\n",
    "            dataY.append(0)\n",
    "        elif article_type_lower == \"entertainment\":\n",
    "            dataY.append(1)\n",
    "        elif article_type_lower == \"politics\":\n",
    "            dataY.append(2)\n",
    "        elif article_type_lower == \"sport\":\n",
    "            dataY.append(3)\n",
    "        elif article_type_lower == \"tech\":\n",
    "            dataY.append(4)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid article type: {}\".format(article_type))\n",
    "        dataX.append(term)\n",
    "\n",
    "dataX = np.array(dataX)\n",
    "dataY = np.array(dataY)\n",
    "\n",
    "print(dataX.shape)\n",
    "print(dataY.shape)\n",
    "print(dataY)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After having parsed the data from the files, split the data into training and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train.shape: (229419,) x_test: (57355,) y_train: (229419,) y_test: (57355,)\n",
      "[3 0 0 ... 3 3 0]\n",
      "[4 1 3 ... 4 2 1]\n"
     ]
    }
   ],
   "source": [
    "x_train,x_test,y_train,y_test = train_test_split(dataX, dataY, test_size=0.2, random_state=1)\n",
    "\n",
    "print(\"x_train.shape:\", x_train.shape, \"x_test:\",x_test.shape, \"y_train:\",y_train.shape, \"y_test:\",y_test.shape)\n",
    "\n",
    "print(y_train)\n",
    "print(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, calculate the priors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12062  8976 12196 11651 12470]\n"
     ]
    }
   ],
   "source": [
    "occurences = np.array([0,0,0,0,0])\n",
    "#index: 0 <=> business, 1 <=> entertainment, 2 <=> politics, 3 <=> sport, 4 <=> tech\n",
    "for i in range(len(y_test)):\n",
    "    occurences[y_test[i]] += 1\n",
    "    \n",
    "priors = occurences\n",
    "\n",
    "print (priors)\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next calculate P(x|Ci) with regularization where alpha = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48056\n",
      "35677\n",
      "48468\n",
      "46990\n",
      "50228\n",
      "[1.2004453  1.26999075 1.19874149 1.20499234 1.19177782]\n"
     ]
    }
   ],
   "source": [
    "probability_matrix = np.zeros((5,9635))\n",
    "\n",
    "for i in range(len(x_train)):\n",
    "    probability_matrix[y_train[i]][x_train[i]-1] += 1\n",
    "\n",
    "for i in range(len(probability_matrix)):\n",
    "    print(len(y_train[y_train == i]))\n",
    "    probability_matrix[i]  = (probability_matrix[i]+1)/(len(y_train[y_train == i]) + 2)\n",
    "\n",
    "print(np.sum(probability_matrix,axis = 1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next predict the x_test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4267631418359341\n"
     ]
    }
   ],
   "source": [
    "y_pred = np.zeros((x_test.shape))\n",
    "\n",
    "for i in range(len(x_test)):\n",
    "    probs = np.array([0,0,0,0,0])\n",
    "\n",
    "    for j in range(5):\n",
    "        probs[j] = priors[j] * probability_matrix[j][x_test[i]-1]\n",
    "    y_pred[i] = np.argmax(probs)\n",
    "\n",
    "accuracy = accuracy_score(y_test,y_pred)\n",
    "\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next predict the x_train_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.458131192272654\n"
     ]
    }
   ],
   "source": [
    "y_pred = np.zeros((x_train.shape))\n",
    "\n",
    "for i in range(len(x_train)):\n",
    "    probs = np.array([0,0,0,0,0])\n",
    "\n",
    "    for j in range(5):\n",
    "        probs[j] = priors[j] * probability_matrix[j][x_train[i]-1]\n",
    "    \n",
    "    y_pred[i] = np.argmax(probs)\n",
    "\n",
    "accuracy = accuracy_score(y_train,y_pred)\n",
    "\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
