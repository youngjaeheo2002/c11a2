{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q3 Part A:\n",
    "\n",
    "1. This is a minimization problem.\n",
    "\n",
    "2. Objective function:\n",
    "\n",
    "minimize - (log g($w^T(4,4,1)^T$) + \\\n",
    "    $\\quad \\quad \\quad \\quad$  log g($w^T(6,4,1)^T$) + \\\n",
    "    $\\quad \\quad \\quad \\quad$  log g($w^T(6,5,1)^T$) + \\\n",
    "    $\\quad \\quad \\quad \\quad$  log [1- g($w^T(6,8,1)^T$)] + \\\n",
    "    $\\quad \\quad \\quad \\quad$  log [1- g($w^T(6,10,1)^T$)] + \\\n",
    "    $\\quad \\quad \\quad \\quad$  log [$g(w^T(8,8,1)^T)$] + \\\n",
    "    $\\quad \\quad \\quad \\quad$  log [1- g($w^T(8,10,1)^T$)])\n",
    "\n",
    "where g(x) = $\\frac{1}{1 + e^{-x}}$ and $w^T$ = [$w_0, w_1, b$]\n",
    "\n",
    "3. Constraints:\n",
    "There are no constraints for logistic regression.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q3 part b\n",
    "\n",
    "Classfication: 1 is orange 0 is not \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.25.0\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterartion 1:\n",
      "New w: [ 0.20477175 -0.35426438  0.68685387]\n",
      "Classification: [1. 1. 1. 1. 1. 1. 1.]\n",
      "accuracy_score: 0.5714285714285714\n",
      "-----------------------------------------\n",
      "Iteration 2:\n",
      "New w [ 0.27913091 -0.3089443   0.69951893]\n",
      "Classification: [1. 1. 1. 0. 0. 0. 0.]\n",
      "accuracy_score: 0.8571428571428571\n",
      "-----------------------------------------\n",
      "Iteration 3:\n",
      "New w [ 0.27208574 -0.35574855  0.69978802]\n",
      "classifications: [1. 1. 1. 0. 0. 1. 0.]\n",
      "accuracy_score: 1.0\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "w_0 = np.array([0.3,-0.2,0.7]).T\n",
    "x = np.array([[4,4,1],[6,4,1],[6,5,1],[6,8,1],[6,10,1],[8,8,1],[8,10,1]])\n",
    "y = np.array([1,1,1,0,0,1,0])\n",
    "def p(w,x):\n",
    "    return  1/(1+math.exp(- w.T @ x))\n",
    "\n",
    "def new_W(oldW,y,y_pred,x):\n",
    "    sum = 0\n",
    "\n",
    "    for i in range(len(y)):\n",
    "        sum += (y[i]-y_pred[i])*x[i].T\n",
    "    return oldW - 0.01*-1 *sum\n",
    "\n",
    "#iter 1\n",
    "print(\"Iterartion 1:\")\n",
    "y_pred = np.zeros((len(x)))\n",
    "for i in range(len(x)):\n",
    "    y_pred[i] = p(w_0,x[i].T)\n",
    "w_1 = new_W(w_0,y,y_pred,x)\n",
    "print(\"New w:\",w_1)\n",
    "y_pred = np.round(y_pred)\n",
    "print(\"Classification:\",y_pred)\n",
    "print(\"accuracy_score:\",accuracy_score(y_pred,y))\n",
    "print(\"-----------------------------------------\")\n",
    "\n",
    "#iter2\n",
    "print(\"Iteration 2:\")\n",
    "y_pred = np.zeros((len(x)))\n",
    "for i in range(len(x)):\n",
    "    y_pred[i] = p(w_1,x[i].T)\n",
    "w_2 = new_W(w_1,y,y_pred,x)\n",
    "print(\"New w\",w_2)\n",
    "y_pred = np.round(y_pred)\n",
    "print(\"Classification:\",y_pred)\n",
    "print(\"accuracy_score:\",accuracy_score(y_pred,y))\n",
    "print(\"-----------------------------------------\")\n",
    "#iter3\n",
    "print(\"Iteration 3:\")\n",
    "y_pred = np.zeros((len(x)))\n",
    "for i in range(len(x)):\n",
    "    y_pred[i] = p(w_2,x[i].T)\n",
    "w_3 = new_W(w_2,y,y_pred,x)\n",
    "print(\"New w\",w_3)\n",
    "\n",
    "\n",
    "y_pred = np.round(y_pred)\n",
    "print(\"classifications:\",y_pred)\n",
    "\n",
    "print(\"accuracy_score:\",accuracy_score(y_pred,y))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part c: Classify new points: [[3,3],[4,10],[9,8],[9,10]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 0. 1. 0.]\n"
     ]
    }
   ],
   "source": [
    "x_test = np.array([[3,3,1],[4,10,1],[9,8,1],[9,10,1]])\n",
    "y_pred = np.empty((4))\n",
    "\n",
    "for i in range(x_test.shape[0]):\n",
    "    y_pred[i] = p(w_3,x_test[i].T)\n",
    "\n",
    "y_pred = np.round(y_pred)\n",
    "\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore, the following points are classified as: \n",
    "\n",
    "(3, 3, orange), (4, 10, not orange), (9, 8, orange), (9, 10, not orange)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part d: \n",
    "\n",
    "One advantage of linear regression is that because the decision boundary is linear, predicting and classifying new data becomes extremely easy as we only need to check if the sigmoid is above the value 0.5. \\\n",
    "One disadvantage is that linear regression can only be reliable in the case that the data is linearly separable"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
