{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we have an ensemble binary classifcation technique with N independent classifiers $\\{C_i\\}_{i=1}^{N}$ where N is odd, and suppose that each classifier has an error rate of $\\epsilon$.\n",
    "\n",
    "Given the classifiers are independent then the errors are not related or correleated, thus in order for a point to be classsified as incorrect the majority of the class labels that were predecited by the base classifiers have to be incorrect. This is because we choose the most commonly selected class label from the base classifiers as our final prediction. So, given our error rate is similar to the probability of getting an error, we must find the rate of getting an incorrect error rate for every time we have a majority of wrong classifiers. To do so, we need to choose some majority of the classifiers, find the error rate of those classifiers, and because the rate is similar to probability, we need to find the correct rate from the remaining classifers to multiply with the error rate.\n",
    "\n",
    "Thus the error must be defined as:\n",
    "\n",
    "Error = $\\sum_{i = \\frac{N}{2} + 1}^{N}$ ${N \\choose i}$ $\\epsilon^{i}$ $(1-\\epsilon)^{N-i}$\n",
    "\n",
    "where:\n",
    "- we need to sum from $\\frac{N}{2} + 1$ to N as we can only misclassify if the majority of the classifiers is incorrect\n",
    "- ${N \\choose i}$ is where we choose some majority of independent classifiers which we misclassify\n",
    "- $\\epsilon^{i}$ is the error rate of all the independent base classifiers (the probability of all the error)\n",
    "- $(1-\\epsilon)^{N-i}$ is the total correct rate of all the classifiers which have the correct labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let p(x|c_i) = $\\frac{1}{\\sigma_i \\sqrt{2\\pi}}$ exp($\\frac{-(x-\\mu_i)^2}{2\\sigma_i^2}$) where i $\\in$ {1,2}. We want to show that if $\\sigma_1 = \\sigma_2$ then the decision boundary is a straight line.\n",
    "\n",
    "Note the decision boundary is defined as a(x) = ln($\\frac{p(x|c_1)p(c_1)}{p(x|c_2)|p(c_2)}$). So we have:\n",
    "\\begin{align*}\n",
    "a(x) & = ln(\\frac{p(x|c_1)p(c_1)}{p(x|c_2)|p(c_2)})\\\\\n",
    "a(x) & = ln(p(x|c_1) p(c_1)) - ln(p(x|c_2)p(c_2)) \\text{\\quad [By log quotient rule]}\\\\\n",
    "a(x) & = ln(p(x|c_1)) + ln(p(c_1)) - ln(p(x|c_2))-ln(p(c_2)) \\text{\\quad [By log product rule]}\\\\\n",
    "\\end{align*}\n",
    "Ignoring ln(p($c_1$)) and ln(p($c_2$)) because they do not depend on x, we have\n",
    "\\begin{align*}\n",
    "a(x) & =  ln(p(x|c_1)) - ln(p(x|c_2))\\\\\n",
    "a(x) & = ln(\\frac{1}{\\sigma_1 \\sqrt{2\\pi}} \\cdot exp(\\frac{-(x-\\mu_1)^2}{2\\sigma_1^2})) - ln(\\frac{1}{\\sigma_2 \\sqrt{2\\pi}} \\cdot exp(\\frac{-(x-\\mu_2)^2}{2\\sigma_2^2})) \\text{\\quad [By def of $p(x|c_i)$]}\\\\\n",
    "a(x) & = ln(\\frac{1}{\\sigma_1 \\sqrt{2\\pi}} \\cdot exp(\\frac{-(x-\\mu_1)^2}{2\\sigma_1^2})) - ln(\\frac{1}{\\sigma_1 \\sqrt{2\\pi}} \\cdot exp(\\frac{-(x-\\mu_2)^2}{2\\sigma_1^2})) \\text{\\quad [Because $\\sigma_1 = \\sigma_2$]}\\\\\n",
    "a(x) & = ln(\\frac{1}{\\sigma_1 \\sqrt{2\\pi}}) + ln(exp(\\frac{-(x-\\mu_1)^2}{2\\sigma_1^2})) - ln(\\frac{1}{\\sigma_1 \\sqrt{2\\pi}}) - ln(exp(\\frac{-(x-\\mu_2)^2}{2\\sigma_1^2})) \\text{\\quad [By log product rules]}\\\\\n",
    "a(x) & = ln(exp(\\frac{-(x-\\mu_1)^2}{2\\sigma_1^2})) - ln(exp(\\frac{-(x-\\mu_2)^2}{2\\sigma_1^2})) \\text{\\quad [By cancelling terms]}\\\\\n",
    "a(x) & = \\frac{-(x-\\mu_1)^2}{2\\sigma_1^2} - \\frac{-(x-\\mu_2)^2}{2\\sigma_1^2} \\text{\\quad [Because $ln(exp^x) = x$]}\\\\\n",
    "a(x) & = \\frac{-1}{2} (x-\\mu_1)^2 \\sigma_1^{-2} + \\frac{1}{2} (x-\\mu_2)^2 \\sigma_1^{-2}\\\\\n",
    "a(x) & = \\frac{-1}{2} \\sigma_1^{-2} ((x-\\mu_1)^2 - (x-\\mu_2)^2)\\\\\n",
    "a(x) & = \\frac{-1}{2} \\sigma_1^{-2} (x^2 - 2x\\mu_1 + \\mu_1^2 - (x^2 - 2x\\mu_2 + \\mu_2^2))\\\\\n",
    "a(x) & = \\frac{-1}{2} \\sigma_1^{-2} (x^2 - 2x\\mu_1 + \\mu_1^2 - x^2 + 2x\\mu_2 - \\mu_2^2)\\\\\n",
    "a(x) & = \\frac{-1}{2} \\sigma_1^{-2} (- 2x\\mu_1 + \\mu_1^2 + 2x\\mu_2 - \\mu_2^2)\\\\\n",
    "\\end{align*}\n",
    "\n",
    "Now, there is no quadratic form of x, therefore then $\\sigma_1 = \\sigma_2$ it is the case that the decision boundary is linear. QED"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
